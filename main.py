from dataclasses import dataclass

import pandas as pd
import numpy as np

from sklearn.impute import SimpleImputer
from sklearn.preprocessing import TargetEncoder, StandardScaler, LabelEncoder
from sklearn.metrics import root_mean_squared_error as RMSE
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader

import matplotlib.pyplot as plt
import seaborn as sns

import warnings
# filter warnings
warnings.filterwarnings('ignore')

df = pd.read_csv('./data/penguins.csv')

print(df.sample(5, random_state=42))

df.info()

# –ó –∫–æ–ª–æ–Ω–∫–∏ Non-Null Count –±–∞—á–∏–º–æ, —â–æ —Ç—ñ–ª—å–∫–∏ –¥–µ–∫—ñ–ª—å–∫–∞ —Ä—è–¥–∫—ñ–≤ –¥–∞–Ω–∏—Ö –º–∞—é—Ç—å –ø—Ä–æ–ø—É—â–µ–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è. –ú–æ–∂–µ–º–æ –≤–∏–¥–∞–ª–∏—Ç–∏ —ó—Ö –∑ –¥–∞—Ç–∞—Å–µ—Ç—É.

df = df.dropna().reset_index(drop=True)
df.info()

# –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ä–æ–∑–ø–æ–¥—ñ–ª –≤–∏–¥—ñ–≤ –≤ –ë–î

plt.figure(figsize=(4,3))
ax = sns.countplot(data=df, x='species')
for i in ax.containers:
    ax.bar_label(i)
    ax.set_xlabel("value")
    ax.set_ylabel("count")
            
plt.suptitle("Target feature distribution")

plt.tight_layout()
plt.show()

# –ü–æ–¥–∏–≤–∏–º–æ—Å—å —Ä–æ–∑–ø–æ–¥—ñ–ª –∫–∞—Ç–µ–≥–æ—Ä—ñ–∞–ª—å–Ω–æ—ó –∑–º—ñ–Ω–Ω–æ—ó island.

plt.figure(figsize=(4,3))
ax = sns.countplot(data=df, x='island')
for i in ax.containers:
    ax.bar_label(i)
    ax.set_xlabel("value")
    ax.set_ylabel("count")
            
plt.suptitle("Island feature distribution")

plt.tight_layout()
plt.show()

# –ü–æ–¥–∏–≤–∏–º–æ—Å—å –Ω–∞ –ø–æ–ø–∞—Ä–Ω–∏–π —Ä–æ–∑–ø–æ–¥—ñ–ª —á–∏—Å–ª–æ–≤–∏—Ö –æ–∑–Ω–∞–∫.

plt.figure(figsize=(6,6))
sns.pairplot(data=df, hue='species').fig.suptitle('Numeric features distribution', y=1)
plt.show()

# –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ–∑–Ω–∞–∫ –º–æ–¥–µ–ª—ñ
features = ['species', 'bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']

df = df.loc[:, features]

df.loc[df['species']=='Adelie', 'species']=0
df.loc[df['species']=='Gentoo', 'species']=1
df.loc[df['species']=='Chinstrap', 'species']=2
df = df.apply(pd.to_numeric)

df.head(2)

# –ü—Ä–µ–¥—Å—Ç–∞–≤–∏–º–æ –º–∞—Ç—Ä–∏—Ü—é –æ–∑–Ω–∞–∫ X —Ç–∞ –≤–µ–∫—Ç–æ—Ä —Ç–∞—Ä–≥–µ—Ç–æ–≤–æ—ó –∑–º—ñ–Ω–Ω–æ—ó y —è–∫ numpy-–º–∞—Å–∏–≤.

X = df.drop('species', axis =1).values
y = df['species'].values
print("df.drop('species', axis =1).values", X)

# –ë–∞—á–∏–º–æ, —â–æ –æ–∑–Ω–∞–∫–∏ –Ω–∞–±–æ—Ä—É –¥–∞–Ω–∏—Ö –¥—É–∂–µ —Ä–æ–∑—Ä—ñ–∑–Ω–µ–Ω—ñ –≤ —Å–≤–æ—î–º—É —á–∏—Å–ª–æ–≤–æ–º—É –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ñ. –©–æ–± –≥–∞—Ä–∞–Ω—Ç—É–≤–∞—Ç–∏, —â–æ –æ–∑–Ω–∞–∫–∏ –±—É–¥—É—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ñ –≤ –æ–¥–Ω–æ–º—É –º–∞—Å—à—Ç–∞–±—ñ, –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—î–º–æ StandardScaler.

scaler = StandardScaler()
X = scaler.fit_transform(X)
print("scaler.fit_transform(X)", X)

# –†–æ–∑–¥—ñ–ª–∏–º–æ –¥–∞–Ω—ñ –Ω–∞ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω—ñ —Ç–∞ —Ç–µ—Å—Ç–æ–≤—ñ.
X_train , X_test , y_train , y_test = train_test_split(X, y, random_state = 42, test_size =0.33, stratify=y)

# –î–ª—è –ø–æ–¥–∞–ª—å—à–æ—ó —Ä–æ–±–æ—Ç–∏ –∑ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫—É PyTorch –ø–µ—Ä–µ—Ç–≤–æ—Ä–∏–º–æ –¥–∞–Ω—ñ –∑ numpy-–º–∞—Å–∏–≤—ñ–≤ —É torch.tensor.

X_train = torch.Tensor(X_train).float()
y_train = torch.Tensor(y_train).long()

X_test = torch.Tensor(X_test).float()
y_test = torch.Tensor(y_test).long()

print(X_train[:1])
print(y_train[:10])

# –î–ª—è –º–∞—Ç—Ä–∏—Ü—ñ –æ–∑–Ω–∞–∫ X –º–∏ –∑–±–µ—Ä–µ–≥–ª–∏ –∑–Ω–∞—á–µ–Ω–Ω—è –∑ –ø–ª–∞–≤–∞—é—á–æ—é –∫–æ–º–æ—é, —É —Ç.—á. –¥–ª—è –≤–µ–∫—Ç–æ—Ä—É —Ç–∞—Ä–≥–µ—Ç—É y –∑–±–µ—Ä–µ–≥–ª–∏ —Ü—ñ–ª–æ—á–∏—Å–µ–ª—å–Ω–∏–π –≤–∏–≥–ª—è–¥. –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è self.long() —î –µ–∫–≤—ñ–≤–∞–ª–µ–Ω—Ç–æ–º self.to(torch.int64), —Ç–æ–±—Ç–æ –ø—Ä–∏–≤–µ–¥–µ–Ω–Ω—è –¥–æ —Ü—ñ–ª–æ—á–∏—Å–µ–ª—å–Ω–æ–≥–æ —Ç–∏–ø—É –¥–∞–Ω–∏—Ö.

#  üí° –ú–æ–¥–∏—Ñ—ñ–∫–æ–≤–∞–Ω—ñ —Ñ—É–Ω–∫—Ü—ñ—ó Leaky ReLU, Parametric ReLU (PReLU) —ñ Exponential Linear Unit (ELU) —Ç–∞–∫–æ–∂ –º–∞—é—Ç—å –≤–ª–∞—Å—Ç–∏–≤–æ—Å—Ç—ñ, —è–∫—ñ —Å–ø—Ä–∏—è—é—Ç—å –∑–∞–ø–æ–±—ñ–≥–∞–Ω–Ω—é –∑–≥–∞—Å–∞–Ω–Ω—è –≥—Ä–∞–¥—ñ—î–Ω—Ç–∞. –ù–∞–ø—Ä–∏–∫–ª–∞–¥, Leaky ReLU –¥–æ–¥–∞—î –Ω–µ–≤–µ–ª–∏–∫–∏–π –Ω–∞—Ö–∏–ª –¥–ª—è –Ω–µ–≥–∞—Ç–∏–≤–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å, –∞ ELU –º–∞—î –µ–∫—Å–ø–æ–Ω–µ–Ω—Ü—ñ–π–Ω—É –∑–∞–ª–µ–∂–Ω—ñ—Å—Ç—å –Ω–µ–≥–∞—Ç–∏–≤–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å.

# –°—Ç–≤–æ—Ä–∏–º–æ –∫–ª–∞—Å –Ω–µ–π—Ä–æ–Ω–Ω–æ—ó –º–µ—Ä–µ–∂—ñ –¥–ª—è –≤–∏—Ä—ñ—à–µ–Ω–Ω—è –∑–∞–¥–∞—á—ñ –±–∞–≥–∞—Ç–æ–∫–ª–∞—Å–æ–≤–æ—ó –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó.

class LinearModel(torch.nn.Module):
    def __init__(self, in_dim, hidden_dim=20, out_dim=3):
        super().__init__()
        
        self.features = torch.nn.Sequential(
            
            nn.Linear(in_dim, hidden_dim),
            torch.nn.ReLU(),
            
            nn.Linear(hidden_dim, out_dim),
            nn.Softmax()
        )    
        
    def forward(self, x):
        output = self.features(x)
        return output
    
    # –û—Ç–∂–µ, –º–∞—î–º–æ –Ω–µ–π—Ä–æ–Ω–Ω—É –º–µ—Ä–µ–∂—É, —è–∫–∞ —Å–∫–ª–∞–¥–∞—î—Ç—å—Å—è –∑ –ª—ñ–Ω—ñ–π–Ω–æ–≥–æ —à–∞—Ä—É, —à–∞—Ä—É –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó ReLU, —Ñ—ñ–Ω–∞–ª—å–Ω–æ–≥–æ –ª—ñ–Ω—ñ–π–Ω–æ–≥–æ —à–∞—Ä—É —Ç–∞ —à–∞—Ä—É Softmax.

    # Softmax ‚Äî —Ü–µ —Ñ—É–Ω–∫—Ü—ñ—è –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó, —è–∫–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –≤ –º–∞—à–∏–Ω–Ω–æ–º—É –Ω–∞–≤—á–∞–Ω–Ω—ñ, –Ω–∞–π—á–∞—Å—Ç—ñ—à–µ –≤ –∑–∞–¥–∞—á–∞—Ö –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó, –¥–ª—è –ø–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è –≤–µ–∫—Ç–æ—Ä—É –∑ –Ω–µ–≤—ñ–¥–Ω–æ—Ä–º–æ–≤–∞–Ω–∏–º–∏ –≤–∏—Ö–æ–¥–∞–º–∏ –Ω–µ–π—Ä–æ–Ω–Ω–æ—ó –º–µ—Ä–µ–∂—ñ (–ª–æ–≥—ñ—Ç–∞–º–∏, logits) —É –≤–µ–∫—Ç–æ—Ä –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç–µ–π. –í–æ–Ω–∞ –∑–∞–±–µ–∑–ø–µ—á—É—î, —â–æ–± —Å—É–º–∞ –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç–µ–π –¥–ª—è –≤—Å—ñ—Ö –∫–ª–∞—Å—ñ–≤ –¥–æ—Ä—ñ–≤–Ω—é–≤–∞–ª–∞ 1.

    # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î–º–æ –º–æ–¥–µ–ª—å.
model = LinearModel(X_train.shape[1], 20, 3)

'''
–ö—ñ–ª—å–∫—ñ—Å—Ç—å –≤—Ö—ñ–¥–Ω–∏—Ö –Ω–µ–π—Ä–æ–Ω—ñ–≤ in_dim –±—É–¥–µ –¥–æ—Ä—ñ–≤–Ω—é–≤–∞—Ç–∏ –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –æ–∑–Ω–∞–∫ –º–∞—Ç—Ä–∏—Ü—ñ 
X. –ü—Ä–æ–º—ñ–∂–Ω–∏–π —à–∞—Ä –º–∞—Ç–∏–º–µ 20 –Ω–µ–π—Ä–æ–Ω—ñ–≤. –í–∏—Ö—ñ–¥–Ω–∏–π —à–∞—Ä –º–∞—Ç–∏–º–µ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –Ω–µ–π—Ä–æ–Ω—ñ–≤, —Ä—ñ–≤–Ω–∏—Ö –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –∫–ª–∞—Å—ñ–≤ - 3.
–í–∏—Ö–æ–¥—è—á–∏ –∑ EDA, –¥–ª—è –≤–∏—Ä—ñ—à–µ–Ω–Ω—è —Ü—ñ—î—ó –∑–∞–¥–∞—á—ñ –±—É–¥–µ –¥–æ—Å—Ç–∞—Ç–Ω—å–æ –Ω–µ–≤–µ–ª–∏–∫–æ—ó –Ω–µ–π—Ä–æ–Ω–Ω–æ—ó –º–µ—Ä–µ–∂—ñ –∑ –Ω–µ–≤–µ–ª–∏–∫–æ—é –∫—ñ–ª—å–∫—ñ—Å—Ç—é –ø—Ä–∏—Ö–æ–≤–∞–Ω–∏—Ö —à–∞—Ä—ñ–≤.

–í —è–∫–æ—Å—Ç—ñ —Ñ—É–Ω–∫—Ü—ñ—ó –≤—Ç—Ä–∞—Ç –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—î–º–æ Cross-Entropy Loss.
üí° Cross-entropy —î –æ–¥–Ω—ñ—î—é –∑ –Ω–∞–π–ø–æ–ø—É–ª—è—Ä–Ω—ñ—à–∏—Ö —Ñ—É–Ω–∫—Ü—ñ–π –≤—Ç—Ä–∞—Ç –¥–ª—è –∑–∞–¥–∞—á –±–∞–≥–∞—Ç–æ–∫–ª–∞—Å–æ–≤–æ—ó –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó. –í–æ–Ω–∞ –æ—Ü—ñ–Ω—é—î –µ—Ñ–µ–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å –º–æ–¥–µ–ª—ñ, –ø–æ—Ä—ñ–≤–Ω—é—é—á–∏ –ø–µ—Ä–µ–¥–±–∞—á—É–≤–∞–Ω—ñ –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ –∫–ª–∞—Å—ñ–≤ –∑ —Ñ–∞–∫—Ç–∏—á–Ω–∏–º–∏ –º—ñ—Ç–∫–∞–º–∏.

–ù–µ–¥–æ–ª—ñ–∫–∏ —Ñ—É–Ω–∫—Ü—ñ—ó –≤—Ç—Ä–∞—Ç Cross-Entropy Loss

- –ß—É—Ç–ª–∏–≤—ñ—Å—Ç—å –¥–æ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∏—Ö –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω—å. –ú–æ–¥–µ–ª—å –º–æ–∂–µ –±—É—Ç–∏ —Å–∏–ª—å–Ω–æ ‚Äú–æ—à—Ç—Ä–∞—Ñ–æ–≤–∞–Ω–∞‚Äù –∑–∞ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ñ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è, –æ—Å–æ–±–ª–∏–≤–æ —è–∫—â–æ –≤–æ–Ω–∞ –≤–ø–µ–≤–Ω–µ–Ω–æ –ø–µ—Ä–µ–¥–±–∞—á–∞—î –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∏–π –∫–ª–∞—Å.
- –ü–æ—Ç—Ä–µ–±–∞ —É –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—ó. –©–æ–± –∫–æ—Ä–µ–∫—Ç–Ω–æ –∑–∞—Å—Ç–æ—Å—É–≤–∞—Ç–∏ —Ñ—É–Ω–∫—Ü—ñ—é –∫—Ä–æ—Å-–µ–Ω—Ç—Ä–æ–ø—ñ—ó, –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –ø–æ–≤–∏–Ω–Ω—ñ –±—É—Ç–∏ –Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ –¥–æ –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç–µ–π (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, —á–µ—Ä–µ–∑ Softmax).
'''
criterion = nn.CrossEntropyLoss()
# –í —è–∫–æ—Å—Ç—ñ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ–π–Ω–æ–≥–æ –∞–ª–≥–æ—Ä–∏—Ç–º—É –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—î–º–æ Stochastic Gradient Descent (SGD).
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
# –ö—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ø–æ—Ö –≤–∏–∑–Ω–∞—á–∏–º–æ —Ä—ñ–≤–Ω–æ—é 400.
num_epoch = 400 
# –°—Ç–≤–æ—Ä–∏–º–æ –¥–µ–∫—ñ–ª—å–∫–∞ –æ–±‚Äô—î–∫—Ç—ñ–≤-—Å–ø–∏—Å–∫—ñ–≤, —É —è–∫—ñ –±—É–¥–µ–º–æ –∑–±–µ—Ä—ñ–≥–∞—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è: accuracy —Ç–∞ loss.
train_loss = []
test_loss = []

train_accs = []
test_accs = []

# –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
model.train()
# –¶–µ –¥–∞—î –º–æ–∂–ª–∏–≤—ñ—Å—Ç—å –æ–±—á–∏—Å–ª—é–≤–∞—Ç–∏ –≥—Ä–∞–¥—ñ—î–Ω—Ç–∏ –º–æ–¥–µ–ª—ñ, –æ–Ω–æ–≤–ª—é–≤–∞—Ç–∏ –≤–∞–≥–∏, –∞ —Ç–∞–∫–æ–∂ –∞–∫—Ç–∏–≤—É—î —Ä—è–¥ —à–∞—Ä—ñ–≤, —â–æ –Ω–µ —î –∞–∫—Ç–∏–≤–Ω–∏–º–∏ –≤ —Ä–µ–∂–∏–º—ñ –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó, –Ω–∞–ø—Ä–∏–∫–ª–∞–¥ Dropout —Ç–∞ BatchNormalization (–ø—Ä–æ –Ω–∏—Ö –ø—ñ–∑–Ω—ñ—à–µ).
# - –í–∏–∫–æ–Ω—É—î–º–æ forward pass, –æ—Ç—Ä–∏–º—É—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ.

outputs = model(X_train)

# - –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ –∑–Ω–∞—á–µ–Ω–Ω—è —Ñ—É–Ω–∫—Ü—ñ—ó –≤—Ç—Ä–∞—Ç —Ç–∞ –∑–±–µ—Ä—ñ–≥–∞—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –ø–æ–¥–∞–ª—å—à–æ—ó –æ–±—Ä–æ–±–∫–∏.
loss = criterion(outputs, y_train)    
train_loss.append(loss.cpu().detach().numpy())

# - –†–æ–±–∏–º–æ –∫—Ä–æ–∫ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó.

optimizer.zero_grad()    
loss.backward()
optimizer.step()

# - –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ —Ç–æ—á–Ω—ñ—Å—Ç—å –º–æ–¥–µ–ª—ñ —Ç–∞ –∑–±–µ—Ä—ñ–≥–∞—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç.

acc = 100 * torch.sum(y_train==torch.max(outputs.data, 1)[1]).double() / len(y_train)
train_accs.append(acc)

# –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è
# - –ü–µ—Ä–µ–≤–µ–¥–µ–º–æ –º–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó.

model.eval()

# üí° –í —Ä–µ–∂–∏–º—ñ –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó –º–∏ –Ω–µ –æ–±—Ä–∞—Ö–æ–≤—É—î–º–æ –≥—Ä–∞–¥—ñ—î–Ω—Ç–∏ —Ç–∞ –Ω–µ –∑–¥—ñ–π—Å–Ω—é—î–º–æ –∑–≤–æ—Ä–æ—Ç–Ω–µ —Ä–æ–∑–ø–æ–≤—Å—é–¥–∂–µ–Ω–Ω—è –ø–æ–º–∏–ª–∫–∏. –¢–∞–∫–æ–∂ –¥–µ—è–∫—ñ —à–∞—Ä–∏, —è–∫ –Ω–∞–ø—Ä–∏–∫–ª–∞–¥ Dropout —Ç–∞ BatchNormalization, –≤—Ç—Ä–∞—á–∞—é—Ç—å —Å–≤–æ—ó –≤–ª–∞—Å—Ç–∏–≤–æ—Å—Ç—ñ. –¶–µ –¥–∞—î –º–æ–∂–ª–∏–≤—ñ—Å—Ç—å –º—ñ–Ω—ñ–º—ñ–∑—É–≤–∞—Ç–∏ —Ä–æ–∑—Ä–∞—Ö—É–Ω–∫–æ–≤—ñ —Ä–µ—Å—É—Ä—Å–∏, –≤–∏—Ç—Ä–∞—Ç–∏ –ø–∞–º‚Äô—è—Ç—ñ, –ø—Ä–∏—à–≤–∏–¥—à—É—î –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –æ–±—Ä–∞—Ö—É–Ω–∫—ñ–≤ —ñ –Ω–µ –¥–∞—î –º–æ–¥–µ–ª—ñ –¥–æ—Å—Ç—É–ø –¥–æ —Ç–µ—Å—Ç–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö.

# - –†–æ–±–∏–º–æ forward pass —Ç–∞ —Ä–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ —Ñ—É–Ω–∫—Ü—ñ—é –≤—Ç—Ä–∞—Ç, –∞–ª–µ –Ω–µ —Ä–æ–±–∏–º–æ –∫—Ä–æ–∫ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó. –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ —Ç–æ—á–Ω—ñ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö.

outputs = model(X_test)
        
loss = criterion(outputs, y_test)
test_loss.append(loss.cpu().detach().numpy())
        
acc = 100 * torch.sum(y_test==torch.max(outputs.data, 1)[1]).double() / len(y_test)
test_accs.append(acc)

for epoch in range(num_epoch):
    
    # train the model
    model.train()
    
    outputs = model(X_train)
    
    loss = criterion(outputs, y_train)    
    train_loss.append(loss.cpu().detach().numpy())
    
    optimizer.zero_grad()    
    loss.backward()
    optimizer.step()
    
    acc = 100 * torch.sum(y_train==torch.max(outputs.data, 1)[1]).double() / len(y_train)
    train_accs.append(acc)
    
    if (epoch+1) % 10 == 0:
        print ('Epoch [%d/%d] Loss: %.4f   Acc: %.4f' 
                       %(epoch+1, num_epoch, loss.item(), acc.item()))
        
    # test the model
    
    model.eval()
    with torch.no_grad():
        outputs = model(X_test)
        
        loss = criterion(outputs, y_test)
        test_loss.append(loss.cpu().detach().numpy())
        
        acc = 100 * torch.sum(y_test==torch.max(outputs.data, 1)[1]).double() / len(y_test)
        test_accs.append(acc)

# –ê–Ω–∞–ª—ñ–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
plt.figure(figsize=(4, 3))
plt.plot(train_loss, label='Train')
plt.plot(test_loss, label='Validation')
plt.legend(loc='best')
plt.xlabel('Epochs')
plt.ylabel('Cross-Entropy Loss')
plt.title('Training vs Validation Loss')
plt.show()

plt.figure(figsize=(4, 3))
plt.plot(train_accs, label='Train')
plt.plot(test_accs, label='Validation')
plt.legend(loc='best')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Training vs Validation Metric')
plt.show()

